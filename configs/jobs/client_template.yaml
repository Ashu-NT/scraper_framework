job:
  id: "client_job_01"
  name: "Client Scraping Job"
  adapter: "your_adapter_key"
  start_url: "https://example.com"
  method: "GET"
  execution_mode: "stream"
  batch_size: 500
  max_pages: 5
  delay_ms: 1000
  dedupe_mode: "BY_SOURCE_URL"
  required_fields: ["source_url"]
  field_schema: ["name", "budget", "posted_ago", "phone", "website"]

  # For dynamic jobs you can pass Selenium parameters here.
  # params:
  #   wait_selector: ".result-card"
  #   wait_time: 15
  #   click_selectors: ["button.load-more"]

enrich:
  enabled: false
  fields: []

processing:
  enabled: true
  schema_version: "1.0"
  stages:
    - plugin: "drop_if_field_empty"
      type: "record"
      on_error: "skip"
      config:
        field: "name"
    - plugin: "field_coverage_analytics"
      type: "analytics"
      on_error: "skip"
      config:
        fields: ["name", "phone", "website"]

sink:
  type: "jsonl"
  path: "output/client_output.jsonl"
  write_mode: "overwrite"

schedule:
  enabled: false
  interval_hours: 24
